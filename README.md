Optimal performance of novel machine learning models often relies on meticulous tuning and optimization of hyperparameters. Conventional approaches such as grid search or random search, in conjunction with cross-validation, are computationally expensive and insufficient for effect/sensitivity inference. We present a framework for hyperparameter optimization (HPO) using experiment design and analysis methods, such as Latin hypercube, response surface, and ANOVA. In preliminary results, the Python implementation (package: hypercube) yielded competitive results against state-of-the-art HPO packages (optuna and hyperopt) in both accuracy and budget.
