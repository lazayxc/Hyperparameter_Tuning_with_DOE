Optimal performance of novel machine learning models often relies on meticulous tuning and optimization of hyperparameters. Conventional approaches such as grid search or random search, in conjunction with cross-validation, are computationally expensive and insufficient for effect/sensitivity inference. We present a framework for hyperparameter optimization (HPO) using experiment design and analysis methods, such as Latin hypercube, response surface, and ANOVA. In preliminary results, the Python implementation (package: hypercube) yielded competitive results against state-of-the-art HPO packages (optuna and hyperopt) in both accuracy and budget.

Report (long!): [a link](https://github.com/lazayxc/hypercube_backup/blob/main/6413_Hypercube.pdf)
Presentation Slides (short!): [a link]([https://github.com/lazayxc/hypercube_backup/blob/main/6413_Hypercube.pdf](https://github.com/lazayxc/hypercube_backup/blob/main/Hypercube_%20a%20DOE-informed%20hyperparameter%20optimization%20machine.pptx)https://github.com/lazayxc/hypercube_backup/blob/main/Hypercube_%20a%20DOE-informed%20hyperparameter%20optimization%20machine.pptx)
